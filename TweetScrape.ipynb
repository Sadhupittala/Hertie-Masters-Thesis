{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c590f54",
   "metadata": {},
   "source": [
    "Notebook for collecting and saving tweets to csv for future use.\n",
    "\n",
    "#### Goal:\n",
    "\n",
    "Is to gather a \"random\" sampling for Biden and Bernie in the 2020 election cycle for sentiment analysis and other things. \n",
    "\n",
    "We will collect 5,000 number of tweets for each day, sorted by relevancy. \n",
    "\n",
    "Things we need to do:\n",
    "- function to increment day after 5,000 is hit\n",
    "- func that'll make the requests (and wait 1 second between each request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c89d655",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f99e6ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c4abd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "bearer_token = os.environ.get(\"BEARER_TOKEN\")\n",
    "\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "\n",
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2FullArchiveSearchPython\"\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1de617c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def increment_day(date):\n",
    "    if not isinstance(date, datetime.date):\n",
    "        raise Error\n",
    "    return date + datetime.timedelta(days=1)\n",
    "\n",
    "def get_query_params(start_time, next_page = None):\n",
    "    params = {}\n",
    "    params['query'] = 'bernie sanders OR #feelthebern OR #bernie2016 lang:en'\n",
    "    params['tweet.fields'] = 'created_at'\n",
    "    params['sort_order'] = 'relevancy'\n",
    "    params['start_time'] = start_time.isoformat()\n",
    "    params['end_time'] = increment_day(start_time).isoformat()\n",
    "    params['max_results'] = 500\n",
    "    if next_page is not None:\n",
    "        params['next_token'] = next_page\n",
    "    \n",
    "    return params\n",
    "\n",
    "def get_data(params):\n",
    "    \n",
    "    res = requests.get(search_url, params=params, auth=bearer_oauth)\n",
    "    \n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7b8a84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bernie Sanders\n",
    "\n",
    "def get_bernie_tweets(start_date, end_date, data = [], next_page = None): \n",
    "\n",
    "    while start_date <= end_date:\n",
    "        ## now we do each day\n",
    "        print(start_date.isoformat())\n",
    "        for i in range(0,10):\n",
    "            # make 10 requests\n",
    "            params = get_query_params(start_date, next_page)\n",
    "            res = get_data(params)\n",
    "            \n",
    "            if res.status_code != 200:\n",
    "                print(res.status_code)\n",
    "                print(res.json())\n",
    "                return data, next_page, start_date\n",
    "            \n",
    "            data += res.json().get('data')\n",
    "            next_page = res.json().get('meta').get('next_token')\n",
    "            time.sleep(1)\n",
    "        \n",
    "        ## clean up variables, increment the day\n",
    "        next_page = None\n",
    "        start_date = increment_day(start_date)\n",
    "    \n",
    "    return data, next_page, start_date\n",
    "                \n",
    "            \n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50a9e91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-01-01T00:00:00+01:00\n",
      "2015-01-02T00:00:00+01:00\n",
      "2015-01-03T00:00:00+01:00\n",
      "2015-01-04T00:00:00+01:00\n",
      "2015-01-05T00:00:00+01:00\n",
      "2015-01-06T00:00:00+01:00\n",
      "2015-01-07T00:00:00+01:00\n",
      "2015-01-08T00:00:00+01:00\n",
      "2015-01-09T00:00:00+01:00\n",
      "2015-01-10T00:00:00+01:00\n",
      "2015-01-11T00:00:00+01:00\n",
      "2015-01-12T00:00:00+01:00\n",
      "2015-01-13T00:00:00+01:00\n",
      "2015-01-14T00:00:00+01:00\n",
      "2015-01-15T00:00:00+01:00\n",
      "2015-01-16T00:00:00+01:00\n",
      "2015-01-17T00:00:00+01:00\n",
      "2015-01-18T00:00:00+01:00\n",
      "2015-01-19T00:00:00+01:00\n",
      "2015-01-20T00:00:00+01:00\n",
      "2015-01-21T00:00:00+01:00\n",
      "2015-01-22T00:00:00+01:00\n",
      "2015-01-23T00:00:00+01:00\n",
      "2015-01-24T00:00:00+01:00\n",
      "2015-01-25T00:00:00+01:00\n",
      "2015-01-26T00:00:00+01:00\n",
      "2015-01-27T00:00:00+01:00\n",
      "2015-01-28T00:00:00+01:00\n",
      "2015-01-29T00:00:00+01:00\n",
      "2015-01-30T00:00:00+01:00\n",
      "2015-01-31T00:00:00+01:00\n",
      "2015-02-01T00:00:00+01:00\n",
      "2015-02-02T00:00:00+01:00\n",
      "2015-02-03T00:00:00+01:00\n",
      "2015-02-04T00:00:00+01:00\n",
      "2015-02-05T00:00:00+01:00\n",
      "2015-02-06T00:00:00+01:00\n",
      "2015-02-07T00:00:00+01:00\n",
      "2015-02-08T00:00:00+01:00\n",
      "2015-02-09T00:00:00+01:00\n",
      "2015-02-10T00:00:00+01:00\n",
      "2015-02-11T00:00:00+01:00\n",
      "2015-02-12T00:00:00+01:00\n",
      "2015-02-13T00:00:00+01:00\n",
      "2015-02-14T00:00:00+01:00\n",
      "2015-02-15T00:00:00+01:00\n",
      "2015-02-16T00:00:00+01:00\n",
      "2015-02-17T00:00:00+01:00\n",
      "2015-02-18T00:00:00+01:00\n",
      "2015-02-19T00:00:00+01:00\n",
      "2015-02-20T00:00:00+01:00\n",
      "429\n",
      "{'title': 'Too Many Requests', 'detail': 'Too Many Requests', 'type': 'about:blank', 'status': 429}\n"
     ]
    }
   ],
   "source": [
    "start_date = datetime.datetime(2015, 1, 1).astimezone()\n",
    "end_date = datetime.datetime(2016, 11, 30).astimezone()\n",
    "\n",
    "res = get_bernie_tweets(start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f1615db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, next_page, start_date = res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "791917aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'created_at': '2015-01-26T18:27:42.000Z',\n",
       " 'id': '559779729029726208',\n",
       " 'text': 'Bernie Sanders Unveils A 12 Point Economic Plan To Break The Koch Oligarchs http://t.co/AMBSIgwuk9 via @politicususa'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75a8d39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2015-02-20T00:00:00+01:00'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_date.isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f9c77c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "keys = data[0].keys()\n",
    "\n",
    "with open('bern2015-02-20.csv', 'w', newline=\"\") as output:\n",
    "    dict_writer = csv.DictWriter(output, keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5208d9",
   "metadata": {},
   "source": [
    "We reached our 15 min limit and are tired....\n",
    "\n",
    "For tomorrow: start_date = 2015-02-20.\n",
    "\n",
    "After that data is collected, read in the csv from today and then concatenate the two and re save to csv. \n",
    "Doesn't make sense? Ask Casey later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9dec620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ebc736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
